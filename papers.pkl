(dp0
S'scholar_data'
p1
(lp2
ccopy_reg
_reconstructor
p3
(cscholar
ScholarArticle
p4
c__builtin__
object
p5
Ntp6
Rp7
(dp8
S'citation_data'
p9
S"@inproceedings{marin2010learning,\n  title={Learning appearance in virtual scenarios for pedestrian detection},\n  author={Marin, Javier and V{\\'a}zquez, David and Ger{\\'o}nimo, David and L{\\'o}pez, Antonio M},\n  booktitle={Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on},\n  pages={137--144},\n  year={2010},\n  organization={IEEE}\n}\n"
p10
sS'attrs'
p11
(dp12
S'url_citation'
p13
(lp14
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:cfskg9wqTe8J:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCcKKRfQVWD3Ah9oJ56jeFjDVtjGO4&scisf=4&ct=citation&cd=0&hl=en
p15
aS'Citation link'
p16
aI9
asS'url_versions'
p17
(lp18
Vhttp://scholar.google.com/scholar?cluster=17243485674852907889&hl=en&as_sdt=0,5
p19
aS'Versions list'
p20
aI8
asS'excerpt'
p21
(lp22
VAbstract Detecting pedestrians in images is a key functionality to avoid vehicle-to-pedestrian collisions. The most promising detectors rely on appearance-based pedestrian classifiers trained with labelled samples. This paper addresses the following question: can a  ...
p23
aS'Excerpt'
p24
aI10
asS'num_citations'
p25
(lp26
I78
aS'Citations'
p27
aI3
asS'cluster_id'
p28
(lp29
V17243485674852907889
p30
aS'Cluster ID'
p31
aI5
asS'year'
p32
(lp33
V2010
p34
aS'Year'
p35
aI2
asS'url_citations'
p36
(lp37
Vhttp://scholar.google.com/scholar?cites=17243485674852907889&as_sdt=2005&sciodt=0,5&hl=en
p38
aS'Citations list'
p39
aI7
asS'num_versions'
p40
(lp41
I16
aS'Versions'
p42
aI4
asS'title'
p43
(lp44
VLearning appearance in virtual scenarios for pedestrian detection
p45
aS'Title'
p46
aI0
asS'url'
p47
(lp48
Vhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=5540218
p49
aS'URL'
p50
aI1
asS'url_pdf'
p51
(lp52
NaS'PDF link'
p53
aI6
assbag3
(g4
g5
Ntp54
Rp55
(dp56
g9
S'@inproceedings{ros2016synthia,\n  title={The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes},\n  author={Ros, German and Sellart, Laura and Materzynska, Joanna and Vazquez, David and Lopez, Antonio M},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={3234--3243},\n  year={2016}\n}\n'
p57
sg11
(dp58
g13
(lp59
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:9QyfedUJYX8J:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCcfF-pgSi4cqVzEExdvO8RiCNgCvj&scisf=4&ct=citation&cd=0&hl=en
p60
ag16
aI9
asg17
(lp61
Nag20
aI8
asg21
(lp62
VAbstract Vision-based semantic segmentation in urban scenarios is a key functionality for autonomous driving. Recent revolutionary results of deep convolutional neural networks (DCNNs) foreshadow the advent of reliable classifiers to perform such visual tasks.  ...
p63
ag24
aI10
asg25
(lp64
I3
ag27
aI3
asg28
(lp65
V9178628328030932213
p66
ag31
aI5
asg32
(lp67
V2016
p68
ag35
aI2
asg36
(lp69
Vhttp://scholar.google.com/scholar?cites=9178628328030932213&as_sdt=2005&sciodt=0,5&hl=en
p70
ag39
aI7
asg40
(lp71
I0
ag42
aI4
asg43
(lp72
VThe synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes
p73
ag46
aI0
asg47
(lp74
Vhttp://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Ros_The_SYNTHIA_Dataset_CVPR_2016_paper.html
p75
ag50
aI1
asg51
(lp76
Nag53
aI6
assbag3
(g4
g5
Ntp77
Rp78
(dp79
g9
S'@article{gaidon2016virtual,\n  title={Virtual Worlds as Proxy for Multi-Object Tracking Analysis},\n  author={Gaidon, Adrien and Wang, Qiao and Cabon, Yohann and Vig, Eleonora},\n  journal={arXiv preprint arXiv:1605.06457},\n  year={2016}\n}\n'
p80
sg11
(dp81
g13
(lp82
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:pNlsupZKwKIJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCcmsUSp8Cpd9P8M8c-_muwisvYFTh&scisf=4&ct=citation&cd=0&hl=en
p83
ag16
aI9
asg17
(lp84
Vhttp://scholar.google.com/scholar?cluster=11727455440906017188&hl=en&as_sdt=0,5
p85
ag20
aI8
asg21
(lp86
VAbstract: Modern computer vision algorithms typically require expensive data acquisition and accurate manual labeling. In this work, we instead leverage the recent progress in computer graphics to generate fully labeled, dynamic, and photo-realistic proxy virtual  ...
p87
ag24
aI10
asg25
(lp88
I3
ag27
aI3
asg28
(lp89
V11727455440906017188
p90
ag31
aI5
asg32
(lp91
V2016
p92
ag35
aI2
asg36
(lp93
Vhttp://scholar.google.com/scholar?cites=11727455440906017188&as_sdt=2005&sciodt=0,5&hl=en
p94
ag39
aI7
asg40
(lp95
I3
ag42
aI4
asg43
(lp96
VVirtual Worlds as Proxy for Multi-Object Tracking Analysis
p97
ag46
aI0
asg47
(lp98
Vhttp://arxiv.org/abs/1605.06457
p99
ag50
aI1
asg51
(lp100
Nag53
aI6
assbag3
(g4
g5
Ntp101
Rp102
(dp103
g9
S'@article{richter2016playing,\n  title={Playing for data: Ground truth from computer games},\n  author={Richter, Stephan R and Vineet, Vibhav and Roth, Stefan and Koltun, Vladlen},\n  journal={arXiv preprint arXiv:1608.02192},\n  year={2016}\n}\n'
p104
sg11
(dp105
g13
(lp106
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:sCWF3mRM9LEJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCdE-oDpBhT0U-Rr8T-f0QqIRK8fdB&scisf=4&ct=citation&cd=0&hl=en
p107
ag16
aI9
asg17
(lp108
Nag20
aI8
asg21
(lp109
VAbstract: Recent progress in computer vision has been driven by high-capacity models trained on large datasets. Unfortunately, creating large datasets with pixel-level labels has been extremely costly due to the amount of human effort required. In this paper, we  ...
p110
ag24
aI10
asg25
(lp111
I1
ag27
aI3
asg28
(lp112
V12822958035144353200
p113
ag31
aI5
asg32
(lp114
V2016
p115
ag35
aI2
asg36
(lp116
Vhttp://scholar.google.com/scholar?cites=12822958035144353200&as_sdt=2005&sciodt=0,5&hl=en
p117
ag39
aI7
asg40
(lp118
I0
ag42
aI4
asg43
(lp119
VPlaying for data: Ground truth from computer games
p120
ag46
aI0
asg47
(lp121
Vhttp://arxiv.org/abs/1608.02192
p122
ag50
aI1
asg51
(lp123
Nag53
aI6
assbag3
(g4
g5
Ntp124
Rp125
(dp126
g9
S'@article{shafaei2016play,\n  title={Play and Learn: Using Video Games to Train Computer Vision Models},\n  author={Shafaei, Alireza and Little, James J and Schmidt, Mark},\n  journal={arXiv preprint arXiv:1608.01745},\n  year={2016}\n}\n'
p127
sg11
(dp128
g13
(lp129
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:a2SyHU5zK98J:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCdR-b9j4PevCSVHFt7_ALy-1GC-49&scisf=4&ct=citation&cd=0&hl=en
p130
ag16
aI9
asg17
(lp131
Nag20
aI8
asg21
(lp132
VAbstract: Video games are a compelling source of annotated data as they can readily provide fine-grained groundtruth for diverse tasks. However, it is not clear whether the synthetically generated data has enough resemblance to the real-world images to  ...
p133
ag24
aI10
asg25
(lp134
I0
ag27
aI3
asg28
(lp135
Nag31
aI5
asg32
(lp136
V2016
p137
ag35
aI2
asg36
(lp138
Nag39
aI7
asg40
(lp139
I0
ag42
aI4
asg43
(lp140
VPlay and Learn: Using Video Games to Train Computer Vision Models
p141
ag46
aI0
asg47
(lp142
Vhttp://arxiv.org/abs/1608.01745
p143
ag50
aI1
asg51
(lp144
Nag53
aI6
assbag3
(g4
g5
Ntp145
Rp146
(dp147
g9
S"@article{kempka2016vizdoom,\n  title={ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning},\n  author={Kempka, Micha{\\l} and Wydmuch, Marek and Runc, Grzegorz and Toczek, Jakub and Ja{\\'s}kowski, Wojciech},\n  journal={arXiv preprint arXiv:1605.02097},\n  year={2016}\n}\n"
p148
sg11
(dp149
g13
(lp150
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:oPjvgvW86zgJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCdxzP6tKKVQleQICfkIm_1BXtTXy0&scisf=4&ct=citation&cd=0&hl=en
p151
ag16
aI9
asg17
(lp152
Vhttp://scholar.google.com/scholar?cluster=4101579648300742816&hl=en&as_sdt=0,5
p153
ag20
aI8
asg21
(lp154
VAbstract: The recent advances in deep neural networks have led to effective vision-based reinforcement learning methods that have been employed to obtain human-level controllers in Atari 2600 games from pixel data. Atari 2600 games, however, do not resemble real- ...
p155
ag24
aI10
asg25
(lp156
I2
ag27
aI3
asg28
(lp157
V4101579648300742816
p158
ag31
aI5
asg32
(lp159
V2016
p160
ag35
aI2
asg36
(lp161
Vhttp://scholar.google.com/scholar?cites=4101579648300742816&as_sdt=2005&sciodt=0,5&hl=en
p162
ag39
aI7
asg40
(lp163
I3
ag42
aI4
asg43
(lp164
VViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning
p165
ag46
aI0
asg47
(lp166
Vhttp://arxiv.org/abs/1605.02097
p167
ag50
aI1
asg51
(lp168
Nag53
aI6
assbag3
(g4
g5
Ntp169
Rp170
(dp171
g9
S'@article{choi2016large,\n  title={A large dataset of object scans},\n  author={Choi, Sungjoon and Zhou, Qian-Yi and Miller, Stephen and Koltun, Vladlen},\n  journal={arXiv preprint arXiv:1602.02481},\n  year={2016}\n}\n'
p172
sg11
(dp173
g13
(lp174
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:wzRPpSCUIFMJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCeLG5yLYpQfiFQsIgQszYZrlTh5UW&scisf=4&ct=citation&cd=0&hl=en
p175
ag16
aI9
asg17
(lp176
Vhttp://scholar.google.com/scholar?cluster=5989950372336055491&hl=en&as_sdt=0,5
p177
ag20
aI8
asg21
(lp178
VAbstract: We have created a dataset of more than ten thousand 3D scans of real objects. To create the dataset, we recruited 70 operators, equipped them with consumer-grade mobile 3D scanning setups, and paid them to scan objects in their environments. The operators  ...
p179
ag24
aI10
asg25
(lp180
I6
ag27
aI3
asg28
(lp181
V5989950372336055491
p182
ag31
aI5
asg32
(lp183
V2016
p184
ag35
aI2
asg36
(lp185
Vhttp://scholar.google.com/scholar?cites=5989950372336055491&as_sdt=2005&sciodt=0,5&hl=en
p186
ag39
aI7
asg40
(lp187
I6
ag42
aI4
asg43
(lp188
VA large dataset of object scans
p189
ag46
aI0
asg47
(lp190
Vhttp://arxiv.org/abs/1602.02481
p191
ag50
aI1
asg51
(lp192
Nag53
aI6
assbag3
(g4
g5
Ntp193
Rp194
(dp195
g9
S'@article{mayer2015large,\n  title={A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation},\n  author={Mayer, Nikolaus and Ilg, Eddy and H{\\"a}usser, Philip and Fischer, Philipp and Cremers, Daniel and Dosovitskiy, Alexey and Brox, Thomas},\n  journal={arXiv preprint arXiv:1512.02134},\n  year={2015}\n}\n'
p196
sg11
(dp197
g13
(lp198
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:rLfh_AVWCeQJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCeWzDGgnZOIE7-e4BSSgD_KC74eX3&scisf=4&ct=citation&cd=0&hl=en
p199
ag16
aI9
asg17
(lp200
Vhttp://scholar.google.com/scholar?cluster=16431759299155441580&hl=en&as_sdt=0,5
p201
ag20
aI8
asg21
(lp202
VAbstract: Recent work has shown that optical flow estimation can be formulated as a supervised learning task and can be successfully solved with convolutional networks. Training of the so-called FlowNet was enabled by a large synthetically generated dataset.  ...
p203
ag24
aI10
asg25
(lp204
I3
ag27
aI3
asg28
(lp205
V16431759299155441580
p206
ag31
aI5
asg32
(lp207
V2015
p208
ag35
aI2
asg36
(lp209
Vhttp://scholar.google.com/scholar?cites=16431759299155441580&as_sdt=2005&sciodt=0,5&hl=en
p210
ag39
aI7
asg40
(lp211
I13
ag42
aI4
asg43
(lp212
VA Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation
p213
ag46
aI0
asg47
(lp214
Vhttp://arxiv.org/abs/1512.02134
p215
ag50
aI1
asg51
(lp216
Nag53
aI6
assbag3
(g4
g5
Ntp217
Rp218
(dp219
g9
S'@inproceedings{su2015render,\n  title={Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views},\n  author={Su, Hao and Qi, Charles R and Li, Yangyan and Guibas, Leonidas J},\n  booktitle={Proceedings of the IEEE International Conference on Computer Vision},\n  pages={2686--2694},\n  year={2015}\n}\n'
p220
sg11
(dp221
g13
(lp222
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:LtDIxfMyyRAJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCe7q0dFuwKBmSN50yS2dhhND8G8cW&scisf=4&ct=citation&cd=0&hl=en
p223
ag16
aI9
asg17
(lp224
Vhttp://scholar.google.com/scholar?cluster=1209553997502402606&hl=en&as_sdt=0,5
p225
ag20
aI8
asg21
(lp226
VAbstract Object viewpoint estimation from 2D images is an essential task in computer vision. However, two issues hinder its progress: scarcity of training data with viewpoint annotations, and a lack of powerful features. Inspired by the growing availability of 3D models, we  ...
p227
ag24
aI10
asg25
(lp228
I29
ag27
aI3
asg28
(lp229
V1209553997502402606
p230
ag31
aI5
asg32
(lp231
V2015
p232
ag35
aI2
asg36
(lp233
Vhttp://scholar.google.com/scholar?cites=1209553997502402606&as_sdt=2005&sciodt=0,5&hl=en
p234
ag39
aI7
asg40
(lp235
I7
ag42
aI4
asg43
(lp236
VRender for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views
p237
ag46
aI0
asg47
(lp238
Vhttp://www.cv-foundation.org/openaccess/content_iccv_2015/html/Su_Render_for_CNN_ICCV_2015_paper.html
p239
ag50
aI1
asg51
(lp240
Nag53
aI6
assbag3
(g4
g5
Ntp241
Rp242
(dp243
g9
S'@article{chang2015shapenet,\n  title={Shapenet: An information-rich 3d model repository},\n  author={Chang, Angel X and Funkhouser, Thomas and Guibas, Leonidas and Hanrahan, Pat and Huang, Qixing and Li, Zimo and Savarese, Silvio and Savva, Manolis and Song, Shuran and Su, Hao and others},\n  journal={arXiv preprint arXiv:1512.03012},\n  year={2015}\n}\n'
p244
sg11
(dp245
g13
(lp246
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:hDjW2atTnhIJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCfGOBZRXxYO4wq_cJlQZ7Qu1sNzv9&scisf=4&ct=citation&cd=0&hl=en
p247
ag16
aI9
asg17
(lp248
Vhttp://scholar.google.com/scholar?cluster=1341601736562194564&hl=en&as_sdt=0,5
p249
ag20
aI8
asg21
(lp250
VAbstract: We present ShapeNet: a richly-annotated, large-scale repository of shapes represented by 3D CAD models of objects. ShapeNet contains 3D models from a multitude of semantic categories and organizes them under the WordNet taxonomy. It is a collection  ...
p251
ag24
aI10
asg25
(lp252
I19
ag27
aI3
asg28
(lp253
V1341601736562194564
p254
ag31
aI5
asg32
(lp255
V2015
p256
ag35
aI2
asg36
(lp257
Vhttp://scholar.google.com/scholar?cites=1341601736562194564&as_sdt=2005&sciodt=0,5&hl=en
p258
ag39
aI7
asg40
(lp259
I8
ag42
aI4
asg43
(lp260
VShapenet: An information-rich 3d model repository
p261
ag46
aI0
asg47
(lp262
Vhttp://arxiv.org/abs/1512.03012
p263
ag50
aI1
asg51
(lp264
Nag53
aI6
assbag3
(g4
g5
Ntp265
Rp266
(dp267
g9
S'@article{vazquez2014virtual,\n  title={Virtual and real world adaptation for pedestrian detection},\n  author={Vazquez, David and Lopez, Antonio M and Marin, Javier and Ponsa, Daniel and Geronimo, David},\n  journal={IEEE transactions on pattern analysis and machine intelligence},\n  volume={36},\n  number={4},\n  pages={797--809},\n  year={2014},\n  publisher={IEEE}\n}\n'
p268
sg11
(dp269
g13
(lp270
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:6Vrh8ZvvmSQJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCflMaL5Qx4XwoSx6tkPuRHCs9DxCf&scisf=4&ct=citation&cd=0&hl=en
p271
ag16
aI9
asg17
(lp272
Vhttp://scholar.google.com/scholar?cluster=2637402509859183337&hl=en&as_sdt=0,5
p273
ag20
aI8
asg21
(lp274
VAbstract\u2014Pedestrian detection is of paramount interest for many applications. Most promising detectors rely on discriminatively learnt classifiers, ie, trained with annotated samples. However, the annotation step is a human intensive and subjective task worth to  ...
p275
ag24
aI10
asg25
(lp276
I46
ag27
aI3
asg28
(lp277
V2637402509859183337
p278
ag31
aI5
asg32
(lp279
V2014
p280
ag35
aI2
asg36
(lp281
Vhttp://scholar.google.com/scholar?cites=2637402509859183337&as_sdt=2005&sciodt=0,5&hl=en
p282
ag39
aI7
asg40
(lp283
I12
ag42
aI4
asg43
(lp284
VVirtual and real world adaptation for pedestrian detection
p285
ag46
aI0
asg47
(lp286
Vhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6587038
p287
ag50
aI1
asg51
(lp288
Nag53
aI6
assbag3
(g4
g5
Ntp289
Rp290
(dp291
g9
S'@inproceedings{aubry2014seeing,\n  title={Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models},\n  author={Aubry, Mathieu and Maturana, Daniel and Efros, Alexei A and Russell, Bryan C and Sivic, Josef},\n  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={3762--3769},\n  year={2014}\n}\n'
p292
sg11
(dp293
g13
(lp294
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:P9d38424OfoJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCf0mreTawflbLke0bO2frpqSe9CdI&scisf=4&ct=citation&cd=0&hl=en
p295
ag16
aI9
asg17
(lp296
Vhttp://scholar.google.com/scholar?cluster=18030645502969108287&hl=en&as_sdt=0,5
p297
ag20
aI8
asg21
(lp298
VAbstract This paper poses object category detection in images as a type of 2D-to-3D alignment problem, utilizing the large quantities of 3D CAD models that have been made publicly available online. Using the&quot; chair&quot; class as a running example, we propose an  ...
p299
ag24
aI10
asg25
(lp300
I103
ag27
aI3
asg28
(lp301
V18030645502969108287
p302
ag31
aI5
asg32
(lp303
V2014
p304
ag35
aI2
asg36
(lp305
Vhttp://scholar.google.com/scholar?cites=18030645502969108287&as_sdt=2005&sciodt=0,5&hl=en
p306
ag39
aI7
asg40
(lp307
I21
ag42
aI4
asg43
(lp308
VSeeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models
p309
ag46
aI0
asg47
(lp310
Vhttp://www.cv-foundation.org/openaccess/content_cvpr_2014/html/Aubry_Seeing_3D_Chairs_2014_CVPR_paper.html
p311
ag50
aI1
asg51
(lp312
Nag53
aI6
assbag3
(g4
g5
Ntp313
Rp314
(dp315
g9
S'@article{zia2013detailed,\n  title={Detailed 3d representations for object recognition and modeling},\n  author={Zia, M Zeeshan and Stark, Michael and Schiele, Bernt and Schindler, Konrad},\n  journal={IEEE transactions on pattern analysis and machine intelligence},\n  volume={35},\n  number={11},\n  pages={2608--2623},\n  year={2013},\n  publisher={IEEE}\n}\n'
p316
sg11
(dp317
g13
(lp318
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:4r8Ggsvyh1sJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCgG27MHvKroFVvEr42KVhmug120eO&scisf=4&ct=citation&cd=0&hl=en
p319
ag16
aI9
asg17
(lp320
Vhttp://scholar.google.com/scholar?cluster=6595507135181144034&hl=en&as_sdt=0,5
p321
ag20
aI8
asg21
(lp322
VAbstract\u2014Geometric 3D reasoning at the level of objects has received renewed attention recently in the context of visual scene understanding. The level of geometric detail, however, is typically limited to qualitative representations or coarse boxes. This is linked to the fact  ...
p323
ag24
aI10
asg25
(lp324
I65
ag27
aI3
asg28
(lp325
V6595507135181144034
p326
ag31
aI5
asg32
(lp327
V2013
p328
ag35
aI2
asg36
(lp329
Vhttp://scholar.google.com/scholar?cites=6595507135181144034&as_sdt=2005&sciodt=0,5&hl=en
p330
ag39
aI7
asg40
(lp331
I13
ag42
aI4
asg43
(lp332
VDetailed 3d representations for object recognition and modeling
p333
ag46
aI0
asg47
(lp334
Vhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6516504
p335
ag50
aI1
asg51
(lp336
Nag53
aI6
assbag3
(g4
g5
Ntp337
Rp338
(dp339
g9
S'@inproceedings{butler2012naturalistic,\n  title={A naturalistic open source movie for optical flow evaluation},\n  author={Butler, Daniel J and Wulff, Jonas and Stanley, Garrett B and Black, Michael J},\n  booktitle={European Conference on Computer Vision},\n  pages={611--625},\n  year={2012},\n  organization={Springer}\n}\n'
p340
sg11
(dp341
g13
(lp342
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:Z-FsZjqw5NEJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCghxJuwOaiBoWSoBNnsgBTEKmg_gh&scisf=4&ct=citation&cd=0&hl=en&scfhb=1
p343
ag16
aI9
asg17
(lp344
Nag20
aI8
asg21
(lp345
VAbstract Ground truth optical flow is difficult to measure in real scenes with natural motion. As a result, optical flow data sets are restricted in terms of size, complexity, and diversity, making optical flow algorithms difficult to train and test on realistic data. We introduce a  ...
p346
ag24
aI10
asg25
(lp347
I221
ag27
aI3
asg28
(lp348
V15124407213489971559
p349
ag31
aI5
asg32
(lp350
V2012
p351
ag35
aI2
asg36
(lp352
Vhttp://scholar.google.com/scholar?cites=15124407213489971559&as_sdt=5,33&sciodt=0,33&hl=en
p353
ag39
aI7
asg40
(lp354
I0
ag42
aI4
asg43
(lp355
VA naturalistic open source movie for optical flow evaluation
p356
ag46
aI0
asg47
(lp357
Vhttp://link.springer.com/chapter/10.1007/978-3-642-33783-3_44
p358
ag50
aI1
asg51
(lp359
Nag53
aI6
assbag3
(g4
g5
Ntp360
Rp361
(dp362
g9
S'@inproceedings{taylor2007ovvv,\n  title={Ovvv: Using virtual worlds to design and evaluate surveillance systems},\n  author={Taylor, Geoffrey R and Chosak, Andrew J and Brewer, Paul C},\n  booktitle={2007 IEEE Conference on Computer Vision and Pattern Recognition},\n  pages={1--8},\n  year={2007},\n  organization={IEEE}\n}\n'
p363
sg11
(dp364
g13
(lp365
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:J6-MwEdABDAJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eChCLTdWULykUlcG7qMhC-K5k1vCqJ&scisf=4&ct=citation&cd=0&hl=en
p366
ag16
aI9
asg17
(lp367
Vhttp://scholar.google.com/scholar?cluster=3459961090644684583&hl=en&as_sdt=0,5
p368
ag20
aI8
asg21
(lp369
VAbstract ObjectVideo Virtual Video (OVVV) is a publicly available visual surveillance simulation test bed based on a commercial game engine. The tool simulates multiple synchronized video streams from a variety of camera configurations, including static, PTZ  ...
p370
ag24
aI10
asg25
(lp371
I58
ag27
aI3
asg28
(lp372
V3459961090644684583
p373
ag31
aI5
asg32
(lp374
V2007
p375
ag35
aI2
asg36
(lp376
Vhttp://scholar.google.com/scholar?cites=3459961090644684583&as_sdt=2005&sciodt=0,5&hl=en
p377
ag39
aI7
asg40
(lp378
I4
ag42
aI4
asg43
(lp379
VOvvv: Using virtual worlds to design and evaluate surveillance systems
p380
ag46
aI0
asg47
(lp381
Vhttp://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4270516
p382
ag50
aI1
asg51
(lp383
Nag53
aI6
assbag3
(g4
g5
Ntp384
Rp385
(dp386
g9
S'@article{qiu2016unrealcv,\n  title={UnrealCV: Connecting Computer Vision to Unreal Engine},\n  author={Qiu, Weichao and Yuille, Alan},\n  journal={arXiv preprint arXiv:1609.01326},\n  year={2016}\n}\n'
p387
sg11
(dp388
g13
(lp389
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:J4t8VxmD1gQJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eChtY5dfKBYAJfcZSltbfYA9SX9FG8&scisf=4&ct=citation&cd=0&hl=en
p390
ag16
aI9
asg17
(lp391
Nag20
aI8
asg21
(lp392
VAbstract: Computer graphics can not only generate synthetic images and ground truth but it also offers the possibility of constructing virtual worlds in which:(i) an agent can perceive, navigate, and take actions guided by AI algorithms,(ii) properties of the worlds can be  ...
p393
ag24
aI10
asg25
(lp394
I0
ag27
aI3
asg28
(lp395
Nag31
aI5
asg32
(lp396
V2016
p397
ag35
aI2
asg36
(lp398
Nag39
aI7
asg40
(lp399
I0
ag42
aI4
asg43
(lp400
VUnrealCV: Connecting Computer Vision to Unreal Engine
p401
ag46
aI0
asg47
(lp402
Vhttp://arxiv.org/abs/1609.01326
p403
ag50
aI1
asg51
(lp404
Nag53
aI6
assbag3
(g4
g5
Ntp405
Rp406
(dp407
g9
S'@article{lerer2016learning,\n  title={Learning Physical Intuition of Block Towers by Example},\n  author={Lerer, Adam and Gross, Sam and Fergus, Rob},\n  journal={arXiv preprint arXiv:1603.01312},\n  year={2016}\n}\n'
p408
sg11
(dp409
g13
(lp410
Vhttp://scholar.googleusercontent.com/scholar.bib?q=info:Wu4t2rhlR7IJ:scholar.google.com/&output=citation&scisig=AAGBfm0AAAAAV9eCh-qEcmd3NrVVIGd6hKBMtxTknSwC&scisf=4&ct=citation&cd=0&hl=en
p411
ag16
aI9
asg17
(lp412
Vhttp://scholar.google.com/scholar?cluster=12846348306706460250&hl=en&as_sdt=0,5
p413
ag20
aI8
asg21
(lp414
VAbstract: Wooden blocks are a common toy for infants, allowing them to develop motor skills and gain intuition about the physical behavior of the world. In this paper, we explore the ability of deep feed-forward models to learn such intuitive physics. Using a 3D game  ...
p415
ag24
aI10
asg25
(lp416
I9
ag27
aI3
asg28
(lp417
V12846348306706460250
p418
ag31
aI5
asg32
(lp419
V2016
p420
ag35
aI2
asg36
(lp421
Vhttp://scholar.google.com/scholar?cites=12846348306706460250&as_sdt=2005&sciodt=0,5&hl=en
p422
ag39
aI7
asg40
(lp423
I2
ag42
aI4
asg43
(lp424
VLearning Physical Intuition of Block Towers by Example
p425
ag46
aI0
asg47
(lp426
Vhttp://arxiv.org/abs/1603.01312
p427
ag50
aI1
asg51
(lp428
Nag53
aI6
assbasS'paper_list'
p429
(lp430
(dp431
S'tag'
p432
S'transfer'
p433
sS'title'
p434
S'Learning appearance in virtual scenarios for pedestrian detection.'
p435
sa(dp436
S'project'
p437
S'http://synthia-dataset.net/'
p438
sS'tag'
p439
S'dataset'
p440
sS'title'
p441
S'The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes.'
p442
sa(dp443
S'project'
p444
S'http://www.xrce.xerox.com/Research-Development/Computer-Vision/Proxy-Virtual-Worlds'
p445
sS'tag'
p446
S'dataset'
p447
sS'title'
p448
S'Virtual Worlds as Proxy for Multi-Object Tracking Analysis.'
p449
sa(dp450
S'project'
p451
NsS'tag'
p452
S'dataset'
p453
sS'title'
p454
S'Playing for data: Ground truth from computer games.'
p455
sa(dp456
S'project'
p457
NsS'tag'
p458
S'dataset'
p459
sS'title'
p460
S'Play and Learn: Using Video Games to Train Computer Vision Models.'
p461
sa(dp462
S'project'
p463
S'http://vizdoom.cs.put.edu.pl/'
p464
sS'code'
p465
S'https://github.com/Marqt/ViZDoom'
p466
sS'tag'
p467
S'tool'
p468
sS'title'
p469
S'ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning.'
p470
sa(dp471
S'project'
p472
S'http://redwood-data.org/3dscan/'
p473
sS'tag'
p474
S'model'
p475
sS'title'
p476
S'A large dataset of object scans.'
p477
sa(dp478
S'project'
p479
NsS'tag'
p480
S'dataset'
p481
sS'title'
p482
S'A Large Dataset to Train Convolutional Networks for Disparity, Optical Flow, and Scene Flow Estimation.'
p483
sa(dp484
S'code'
p485
S'https://github.com/ShapeNet/RenderForCNN'
p486
sS'tag'
p487
S'tool, transfer'
p488
sS'title'
p489
S'Render for cnn: Viewpoint estimation in images using cnns trained with rendered 3d model views.'
p490
sa(dp491
S'project'
p492
S'http://shapenet.cs.stanford.edu/'
p493
sS'tag'
p494
S'model'
p495
sS'title'
p496
S'Shapenet: An information-rich 3d model repository.'
p497
sa(dp498
S'tag'
p499
S'transfer'
p500
sS'title'
p501
S'Virtual and real world adaptation for pedestrian detection.'
p502
sa(dp503
S'project'
p504
S'http://www.di.ens.fr/willow/research/seeing3Dchairs/'
p505
sS'code'
p506
S'https://github.com/dimatura/seeing3d'
p507
sS'title'
p508
S'Seeing 3d chairs: exemplar part-based 2d-3d alignment using a large dataset of cad models.'
p509
sa(dp510
S'title'
p511
S'Detailed 3d representations for object recognition and modeling.'
p512
sa(dp513
S'project'
p514
S'http://sintel.is.tue.mpg.de/'
p515
sS'tag'
p516
S'optical flow, sintel, dataset'
p517
sS'cluster_id'
p518
L15124407213489971559L
sS'title'
p519
S'A naturalistic open source movie for optical flow evaluation.'
p520
sa(dp521
S'title'
p522
S'Ovvv: Using virtual worlds to design and evaluate surveillance systems.'
p523
sa(dp524
S'comment'
p525
S'My personal project'
p526
sS'code'
p527
S'http://unrealcv.github.io'
p528
sS'tag'
p529
S'tool, diagnosis'
p530
sS'title'
p531
S'UnrealCV: Connecting Computer Vision to Unreal Engine'
p532
sa(dp533
S'code'
p534
S'https://github.com/facebook/UETorch'
p535
sS'tag'
p536
S'tool'
p537
sS'title'
p538
S'Learning Physical Intuition of Block Towers by Example'
p539
sas.